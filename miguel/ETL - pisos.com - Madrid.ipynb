{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b5c1ce",
   "metadata": {},
   "source": [
    "# PISOS.COM\n",
    "\n",
    "Extracción mediante técnicas de Web Scraping para obtener información sobre el mercado inmobiliario de la Comunidad de Madrid, organizado por zonas.\n",
    "\n",
    "- https://www.pisos.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6496ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbccc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver de Chrome\n",
    "\n",
    "chrome_driver = \"C:/Users/Aa/Desktop/MIGUEL/HACK A BOSS/Bootcamp/CLASES/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1bf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar webdriver.Chrome abrirá una ventana nueva en el navegador \n",
    "# A traves de esta ventana podremos seguir usando selenium\n",
    "\n",
    "browser = webdriver.Chrome(executable_path = chrome_driver)\n",
    "\n",
    "browser.get(\"https://www.pisos.com/viviendas/madrid/\")\n",
    "\n",
    "browser.maximize_window()\n",
    "\n",
    "sleep(2)\n",
    "\n",
    "browser.find_element(by = By.CSS_SELECTOR, value = \"#didomi-notice-agree-button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea843c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que ir por zonas de Madrid e incluso por barrios para obtener la mayor cantidad de información, habría que mezclar\n",
    "# Selenium con BeautifulSoup para obtener lo que necesitamosabs\n",
    "\n",
    "# Al final de cada zona desaparece el boton de \"Siguiente\" así que podríamos usar esto para saber si hemos llegado al final\n",
    "# De no hacerlo, la máquina podría estar cambiando de páginas hasta llegar a la 99 pero repitiendo contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df36076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zonas_madrid = {\"madrid_norte\" : \"#MapMap_3\",\n",
    "                \"madrid_noroeste\" : \"#MapMap_2\",\n",
    "                \"madrid_suroeste\" : \"#MapMap_6\",\n",
    "                \"madrid_sur\" : \"#MapMap_4\",\n",
    "                \"madrid_sureste\" : \"#MapMap_5\",\n",
    "                \"corredor_del_henares\" : \"#MapMap_0\",\n",
    "                \"madrid_capital\" : \"#MapMap_1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto ya podemos usar BeautifulSoup para sacar la información.\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad6566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28f36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_paginas = []\n",
    "\n",
    "for prov in provincias:\n",
    "\n",
    "    web = f\"https://www.pisos.com/venta/pisos-{prov}/\"\n",
    "\n",
    "    response = requests.get(web)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    print(f\"{prov} : {response.status_code}\")\n",
    "    \n",
    "    urls_paginas.append(web)\n",
    "    \n",
    "    cont = 2\n",
    "    pag = 2\n",
    "\n",
    "    while True:\n",
    "\n",
    "        url = f\"{web}{pag}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.url in urls_paginas:\n",
    "            break\n",
    "\n",
    "        urls_paginas.append(url)\n",
    "\n",
    "        print(f\"Página {cont} - TERMINADA\")\n",
    "\n",
    "        pag += 1\n",
    "        cont += 1\n",
    "\n",
    "        sleep(1)\n",
    "        \n",
    "    print(\"--\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea57129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Llenamos la lista de URLs sin saber cuántas páginas son en total\n",
    "\n",
    "urls_paginas = [web]\n",
    "cont = 2\n",
    "pag = 2\n",
    "\n",
    "while True:\n",
    "    \n",
    "    url = f\"{web}{pag}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.url in urls_paginas:\n",
    "        break\n",
    "    \n",
    "    urls_paginas.append(url)\n",
    "    \n",
    "    print(f\"Página {cont} - TERMINADA\")\n",
    "    \n",
    "    pag += 1\n",
    "    cont += 1\n",
    "    \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f1414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_paginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c136f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenamos una lista con todos los enlaces a todos los pisos\n",
    "\n",
    "urls_pisos = []\n",
    "\n",
    "for enum, url in enumerate(urls_paginas[], 1):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    for ind, piso in enumerate(soup.find_all(\"div\", class_ = \"ad-preview__bottom\"), 1):\n",
    "        \n",
    "        urls_pisos.append(piso.find(\"a\")[\"href\"])\n",
    "        # print(f\"Piso {ind} de la página {enum} TERMINADO\")\n",
    "        \n",
    "    print(f\"Página {enum} - TERMINADA\")\n",
    "#     print(\"-\"*100)\n",
    "        \n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ebc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_pisos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_cadena(lista):\n",
    "    cadena = \"\"\n",
    "    for i in lista:\n",
    "        if \":\" in i:\n",
    "            cadena = cadena + i + \". \"\n",
    "        else:\n",
    "            cadena = cadena + i\n",
    "    return cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = []\n",
    "zonas = []\n",
    "construidos = []\n",
    "# utiles = []\n",
    "habitaciones = []\n",
    "baños = []\n",
    "plantas = []\n",
    "# años = []\n",
    "# estados = []\n",
    "precios_metro_cuadrado = []\n",
    "precios_venta = []\n",
    "descripciones = []\n",
    "datos_basicos = []\n",
    "muebles_acabados = []\n",
    "equip_instalaciones = []\n",
    "exteriores = []\n",
    "\n",
    "for enum, url in enumerate(urls_pisos, 1):\n",
    "    \n",
    "    response = requests.get(f\"https://www.pisos.com{url}\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "###################################################################################################################\n",
    "    try:\n",
    "        titulo = soup.find(\"h1\", class_ = \"title\").text\n",
    "    except:\n",
    "        titulo = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        zona = soup.find(\"h2\", class_ = \"position\").text\n",
    "    except:\n",
    "        zona = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        cons = soup.find(\"div\", class_ = \"basicdata-info\").text.strip().split()[0]\n",
    "    except:\n",
    "        cons = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "#     try:\n",
    "#         ut = \n",
    "#     except:\n",
    "#         ut = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        hab = soup.find(\"div\", class_ = \"basicdata-info\").text.strip().split()[2]\n",
    "    except:\n",
    "        hab = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        baño = soup.find(\"div\", class_ = \"basicdata-info\").text.strip().split()[4]\n",
    "    except:\n",
    "        baño = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        planta = soup.find(\"div\", class_ = \"basicdata-info\").text.strip().split()[5][-2:]\n",
    "    except:\n",
    "        planta = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "#     try:\n",
    "#         año = \n",
    "#     except:\n",
    "#         año = np.nan\n",
    "        \n",
    "###################################################################################################################\n",
    "#     try:\n",
    "#         estado = \n",
    "#     except:\n",
    "#         estado = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        precio_m2 = soup.find(\"div\", class_ = \"basicdata-info\").text.strip().split()[7]\n",
    "    except:\n",
    "        precio_m2 = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        precio = soup.find(\"span\", class_ = \"h1 jsPrecioH1\").text.split()[0]\n",
    "    except:\n",
    "        precio = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        descripcion = soup.find(\"div\", class_ = \"description-container description-body\").text.strip()\n",
    "    except:\n",
    "        descripcion = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        datos = soup.find(\"ul\", class_ = \"charblock-list charblock-basics\").text.strip().split(\"\\n\")\n",
    "        datos = hacer_cadena(datos)\n",
    "    except:\n",
    "        datos = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        acab = soup.find(\"li\", class_ = \"charblock-element element-with-bullet\").text.strip().split(\"\\n\")\n",
    "        acab = hacer_cadena(acab)\n",
    "    except:\n",
    "        acab = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        inst = soup.find_all(\"div\", class_ = \"charblock-right\")[2].text.strip().split(\"\\n\")\n",
    "        inst = hacer_cadena(inst)\n",
    "    except:\n",
    "        inst = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "    try:\n",
    "        ext = soup.find_all(\"div\", class_ = \"charblock-right\")[3].text.strip().split(\"\\n\")\n",
    "        ext = hacer_cadena(ext)\n",
    "    except:\n",
    "        ext = np.nan\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "    titulos.append(titulo)\n",
    "    zonas.append(zona)\n",
    "    construidos.append(cons)\n",
    "#     utiles.append(ut)\n",
    "    habitaciones.append(hab)\n",
    "    baños.append(baño)\n",
    "    plantas.append(planta)\n",
    "#     años.append(año)\n",
    "#     estados.append(estado)\n",
    "    precios_m2.append(precio_m2)\n",
    "    precios_venta.append(precio)\n",
    "    descripciones.append(descripcion)\n",
    "    datos_basicos.append(datos)\n",
    "    muebles_acabados.append(acab)\n",
    "    equip_instalaciones.append(inst)\n",
    "    exteriores.append(ext)\n",
    "    \n",
    "    print(f\"Piso {enum} TERMINADO\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Titulo\"] = titulos\n",
    "df[\"Zona\"] = zonas\n",
    "df[\"Metros Construidos\"] = construidos\n",
    "df[\"Habitaciones\"] = habitaciones\n",
    "df[\"Baños\"] = baños\n",
    "df[\"Planta\"] = plantas\n",
    "df[\"Precio x m2\"] = precios_m2\n",
    "df[\"Precio\"] = precios_venta\n",
    "df[\"Descripcion\"] = descripciones\n",
    "df[\"Informacion\"] = datos_basicos\n",
    "df[\"Acabados\"] = muebles_acabados\n",
    "df[\"Instalaciones\"] = equip_instalaciones\n",
    "df[\"Exteriores\"] = exteriores\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d70a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13d618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9fa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889480f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
