{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_underscore(names_list):\n",
    "    cleaned_list = []\n",
    "    for name in names_list:\n",
    "        # Remove any sequence within parentheses including the parentheses\n",
    "        name_no_parenthesis = re.sub(r'\\(.*?\\)', '', name).strip()\n",
    "        # Convert to ASCII, remove tildes, and replace spaces/dashes with underscores\n",
    "        cleaned_name = re.sub(r'[\\s\\-]+', '_', unidecode(name_no_parenthesis))\n",
    "        cleaned_list.append(f\"pisos-{cleaned_name}/\")\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "def getOffersFrom(url):\n",
    "    soups = []\n",
    "    for i in range(1,101):\n",
    "        endpoint = f'{url}{i}/'\n",
    "        response = requests.get(endpoint)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return soups\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        no_results = soup.find('div', class_ = 'no-results')\n",
    "        \n",
    "        if no_results:\n",
    "            return soups\n",
    "        \n",
    "        soups.append(soup.body)\n",
    "        print(f'Scraped page {i}')\n",
    "        #sleep(0.5)\n",
    "    return soups\n",
    "\n",
    "def getUrls(soups):\n",
    "    ads = []\n",
    "    for soup in soups:\n",
    "        ad = [x.find('a', class_='ad-preview__title')['href'] for x in soup.find('div', class_ = 'grid__wrapper').find_all('div', class_ = 'ad-preview')]\n",
    "        ads.extend(ad)\n",
    "    return ads\n",
    "\n",
    "def getPrice(soup):\n",
    "    try:\n",
    "        price = soup.find('div', class_ = 'maindata').find('div', class_ = 'priceBox-price').text.strip()\n",
    "    except:\n",
    "        price = np.nan\n",
    "    return price\n",
    "\n",
    "def getTitle(soup):\n",
    "    try:\n",
    "        title = soup.find('div', class_ = 'maindata').find('h1', class_ = 'title').text.strip()\n",
    "    except:\n",
    "        title = np.nan\n",
    "    return title\n",
    "\n",
    "def getLocation(soup):\n",
    "    try:\n",
    "        location = soup.find('div', id = 'location').find('div', class_ = 'location').find('div', class_ = 'subtitle').text.strip()\n",
    "    except:\n",
    "        location = np.nan\n",
    "    return location\n",
    "\n",
    "def getLatLong(soup):\n",
    "    try:\n",
    "        lat = soup.find('div', id = 'location').find('script', type = 'text/javascript').text.strip().split(';')[0].split('=')[-1].strip()\n",
    "        long = soup.find('div', id = 'location').find('script', type = 'text/javascript').text.strip().split(';')[1].split('=')[-1].strip()\n",
    "    except:\n",
    "        lat, long = np.nan, np.nan\n",
    "    return lat, long\n",
    "\n",
    "def getCharacteristics(soup):\n",
    "    try:\n",
    "        charblocks = soup.find('div', class_ = 'characteristics').find_all('div', class_ = 'charblock')\n",
    "        characteristics = []\n",
    "        for charblock in charblocks:\n",
    "            characteristics.extend(charblock.find_all('li'))\n",
    "            \n",
    "        characteristics = [' '.join(x.text.split('\\n')).strip() for x in characteristics]\n",
    "    except:\n",
    "        characteristics = np.nan\n",
    "    return characteristics\n",
    "\n",
    "def scrapeOnePage(url):\n",
    "\n",
    "    chrome_driver = f\"{os.getcwd()}/chromedriver\"\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path = chrome_driver)\n",
    "\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.maximize_window()\n",
    "\n",
    "    element = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]'))\n",
    "    )\n",
    "\n",
    "    element.click() # Accept cookies\n",
    "\n",
    "#     def wait_for_scroll_to_finish(browser, expected_position):\n",
    "#         while True:\n",
    "#             current_position = browser.execute_script(\"return window.pageYOffset;\")\n",
    "#             if current_position >= expected_position - 5:  # A small threshold to account for any small discrepancies\n",
    "#                 break\n",
    "#             sleep(0.1)  # Check every 100ms\n",
    "\n",
    "#     total_height = browser.execute_script(\"return document.body.scrollHeight;\")\n",
    "#     intervals = random.randint(2, 5)  # You can adjust this range based on your preference\n",
    "\n",
    "#     for i in range(1, intervals):\n",
    "#         partial_scroll = total_height * i / intervals\n",
    "#         browser.execute_script(f\"window.scroll({{ top: {partial_scroll}, behavior: 'smooth' }});\")\n",
    "\n",
    "#         wait_for_scroll_to_finish(browser, partial_scroll)\n",
    "\n",
    "#         waiting = random.uniform(0, 0.3)\n",
    "#         sleep(waiting)  # Adjust sleep as necessary for each scroll to complete\n",
    "\n",
    "    while True:\n",
    "        is_at_bottom = browser.execute_script(\"return window.scrollY + window.innerHeight >= document.body.scrollHeight\")\n",
    "        browser.execute_script(\"window.scroll({ top: document.body.scrollHeight, behavior: 'smooth' });\")\n",
    "        if is_at_bottom:\n",
    "            break\n",
    "        sleep(1)\n",
    "\n",
    "    html_content = browser.page_source\n",
    "\n",
    "    browser.quit()\n",
    "    \n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "\n",
    "base_url = 'https://www.pisos.com/'\n",
    "\n",
    "venta_url = f'{base_url}venta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacamos todas las paginas filtradas por provincias\n",
    "\n",
    "response = requests.get(base_url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "provincias_str = soup.find('div', class_ = 'home-container').find('div', class_ = 'selectBox').find_all('ul')[-1].text\n",
    "\n",
    "provincias = [x.lower() for x in provincias_str.strip().split('\\n')]\n",
    "\n",
    "endpoints = replace_with_underscore(provincias)\n",
    "\n",
    "endpoints = [venta_url + endpoint for endpoint in endpoints]\n",
    "\n",
    "with open('locations.txt', 'a+') as file:\n",
    "    for endpoint in endpoints:\n",
    "        file.writelines(endpoint + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE PAGE SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape(url, limit = None):\n",
    "    data = {'title' : [],\n",
    "            'price' : [],\n",
    "            'location' : [],\n",
    "            'lat' : [],\n",
    "            'long' : [],\n",
    "            'characteristics' : [],\n",
    "           }\n",
    "    \n",
    "    chrome_driver = f\"{os.getcwd()}/chromedriver.exe\"\n",
    "\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    browser.maximize_window()\n",
    "\n",
    "    element = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]'))\n",
    "    )\n",
    "\n",
    "    element.click() # Accept cookies\n",
    "\n",
    "    first_page_html = browser.page_source\n",
    "    soup = BeautifulSoup(first_page_html, 'html.parser')\n",
    "    first_ad = soup.find('div', class_ = 'ad-preview').find('a', class_ = 'ad-preview__title')['href']\n",
    "    first_url = base_url + first_ad\n",
    "    browser.quit()\n",
    "\n",
    "\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(first_url)\n",
    "    browser.maximize_window()\n",
    "\n",
    "    element = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]'))\n",
    "    )\n",
    "\n",
    "    element.click() # Accept cookies\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        if limit is not None:\n",
    "            if counter == limit:\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            is_at_bottom = browser.execute_script(\"return window.scrollY + window.innerHeight >= document.body.scrollHeight\")\n",
    "            browser.execute_script(\"window.scroll({ top: document.body.scrollHeight, behavior: 'smooth' });\")\n",
    "            if is_at_bottom:\n",
    "                break\n",
    "            sleep(0.2)\n",
    "        \n",
    "        html_content = browser.page_source\n",
    "        location = 'test'\n",
    "\n",
    "        file_path = f'{os.getcwd()}/html_content/{location}_{counter}.html'\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        next_page_element = WebDriverWait(browser, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"lnkNextPreview\"]'))\n",
    "        )\n",
    "\n",
    "        next_page_element.click() # Click on next page\n",
    "        \n",
    "        counter += 1\n",
    "    browser.quit()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "test_url = endpoints[0]\n",
    "\n",
    "limit = 10\n",
    "\n",
    "data = scrape(test_url, limit = limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
