{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import plotly.express as px\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from locations import *\n",
    "from parsing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES GLOBALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROVINCE = 'malaga'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTES GLOBALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "\n",
    "base_url = 'https://www.pisos.com/'\n",
    "\n",
    "venta_url = f'{base_url}venta/'\n",
    "\n",
    "URL = f'https://www.pisos.com/viviendas/{PROVINCE}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE PAGE SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(urls):\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    for idx, url in enumerate(urls):\n",
    "        browser.get(url)\n",
    "        # browser.maximize_window()\n",
    "\n",
    "        if idx == 0:\n",
    "            element = WebDriverWait(browser, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"didomi-notice-agree-button\"]'))\n",
    "            )\n",
    "\n",
    "            element.click() # Accept cookies\n",
    "\n",
    "        # <SCROLLING>\n",
    "        while True:\n",
    "            is_at_bottom = browser.execute_script(\"return window.scrollY + window.innerHeight >= document.body.scrollHeight\")\n",
    "            browser.execute_script(\"window.scroll({ top: document.body.scrollHeight, behavior: 'smooth' });\")\n",
    "            if is_at_bottom:\n",
    "                break\n",
    "            sleep(0.2)\n",
    "        # </SCROLLING>\n",
    "        \n",
    "        html_content = browser.page_source\n",
    "\n",
    "        timestamp = ''.join(str(datetime.datetime.now().timestamp()).split('.'))\n",
    "\n",
    "        soft_url = url.replace('https://www.pisos.com/comprar/', '')\n",
    "        soft_url = soft_url.replace('/', '_')\n",
    "        soft_url = soft_url.replace('-', '_')\n",
    "\n",
    "        file_path = f'../html_content/{timestamp}_{soft_url}.html'\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(html_content)\n",
    "        except Exception as e:\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(repr(e))\n",
    "\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanRegions(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    print(f'URL: {url} | STATUS {response.status_code}')\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    items = soup.select('div.zoneList a.item:not(.item-subitem)')\n",
    "\n",
    "    endpoints = {}\n",
    "    for item in items:\n",
    "        endpoint = item['href']\n",
    "        n_results = item.find('span', class_ = 'total').text\n",
    "\n",
    "        # <INT>\n",
    "        if len(n_results) != 0:\n",
    "            try:\n",
    "                n_results = n_results[1:-1]\n",
    "                n_results = ''.join(n_results.split('.'))\n",
    "                n_results = int(n_results)\n",
    "            except:\n",
    "                print(f'FAIL CASTING TO INTEGER {endpoint}')\n",
    "        else:\n",
    "            print(f'n_results EMPTY {endpoint}')\n",
    "        #</INT>\n",
    "\n",
    "        # <RECURSSION>\n",
    "        if n_results > 3000:\n",
    "            endpoints[endpoint] = scanRegions(base_url[:-1] + endpoint)\n",
    "        # </RECURSSION>\n",
    "\n",
    "        else: endpoints[endpoint] = n_results\n",
    "\n",
    "    return endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRegions(endpoints):\n",
    "\n",
    "    def extract(endpoints):\n",
    "        array = []\n",
    "        for key, value in endpoints.items():\n",
    "            if isinstance(value, int):\n",
    "                array.append(key)\n",
    "                continue\n",
    "\n",
    "            data = extract(value)\n",
    "            array.extend(data)\n",
    "\n",
    "        return array\n",
    "    \n",
    "    endpoints = extract(endpoints)\n",
    "\n",
    "    array = []\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        data = endpoint\n",
    "        if '/venta/pisos-' in endpoint:\n",
    "            data = endpoint.replace('/venta/pisos-', '/viviendas/')\n",
    "        array.append(data)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = scanRegions(URL)\n",
    "# urls = parseRegions(urls)\n",
    "# urls = [base_url + x for x in urls]\n",
    "# urls = [x.replace('//viviendas/', '/venta/pisos-') for x in urls]\n",
    "# print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeUrls(endpoint):\n",
    "    response = requests.get(endpoint)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    results = soup.find('div', class_ = 'grid__title').find_all('span')[-1].text\n",
    "\n",
    "    n_results = int(''.join([x for x in results if x.isnumeric()]))\n",
    "    n_pages = (n_results // 30) + 1\n",
    "\n",
    "    urls = []\n",
    "    for i in range(n_pages):\n",
    "        url = f'{endpoint}{i + 1}'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        ads = soup.find_all('a', class_ = 'ad-preview__title')\n",
    "        urls.extend([x['href'] for x in ads])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in urls:\n",
    "#     urls_ = scrapeUrls(url)\n",
    "#     urls_ = list(set(urls_))\n",
    "#     urls_ = [base_url[:-1] + x for x in urls_]\n",
    "#     with open('urls.csv', 'a+') as file:\n",
    "#         file.writelines([x + ',\\n' for x in urls_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.csv') as file:\n",
    "    urls = file.read()\n",
    "\n",
    "urls = urls.split(',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape(urls[24686:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la Data a partir de los archivos de la carpeta 'html_content'\n",
    "\n",
    "data = {\n",
    "    'price' : [],\n",
    "    'title' : [],\n",
    "    'province' : [],\n",
    "    'location' : [],\n",
    "    'lat' : [],\n",
    "    'lng' : [],\n",
    "    'characteristics' : [],\n",
    "    'agency' : [],\n",
    "    'updated' : [],\n",
    "    'numeric_data' : [],\n",
    "}\n",
    "\n",
    "files = glob.glob('../html_content/*.html')\n",
    "\n",
    "for file in files:\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        source = f.read()\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    price = getPrice(soup)\n",
    "    title = getTitle(soup)\n",
    "    location = getLocation(soup)\n",
    "    lat, long = getLatLong(soup)\n",
    "    characteristics = getCharacteristics(soup)\n",
    "    updated, agency = getAgencyDate(soup)\n",
    "    numeric_data = [x for x in file[16:].split('_') if x.isnumeric()]\n",
    "\n",
    "    data['price'].append(price)\n",
    "    data['title'].append(title)\n",
    "    data['province'].append(PROVINCE)\n",
    "    data['location'].append(location)\n",
    "    data['lat'].append(lat)\n",
    "    data['lng'].append(long)\n",
    "    data['characteristics'].append(characteristics)\n",
    "    data['agency'].append(agency)\n",
    "    data['updated'].append(updated)\n",
    "    data['numeric_data'].append(numeric_data)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'../data/{PROVINCE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
